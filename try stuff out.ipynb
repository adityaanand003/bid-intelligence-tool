{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57169a46-b647-4e88-a7f7-0ff469d8775a",
   "metadata": {},
   "source": [
    "# Data Extraction "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb5af29-d46c-46e6-88eb-4d91afda0a0d",
   "metadata": {},
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a8b27f3b-3088-439d-b754-d4b04a1b4a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "from pathlib import Path\n",
    "import google.generativeai as genai\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ada3ff-0c73-483c-ab1e-019376ba27ed",
   "metadata": {},
   "source": [
    "### functions to extract text from pds and html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ffe8141e-6cad-4bc0-9fde-188310799ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdf\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    try:\n",
    "        text = \"\"\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for page in pdf.pages:\n",
    "                text = text + page.extract_text(x_tolerance=2, y_tolerance=2) + \"\\n\"\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return \"\"\n",
    "# html\n",
    "def extract_text_from_html(html_path):\n",
    "    try:\n",
    "        with open(html_path, 'r', encoding='utf-8') as f:\n",
    "            soup = BeautifulSoup(f, 'lxml')\n",
    "            text = soup.get_text(separator='\\n', strip=True)\n",
    "            return text\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769d056a-25a4-4979-b932-ab02b8dcd75c",
   "metadata": {},
   "source": [
    "### function to iterate through a folder and grab text from files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fbb1b1f5-e8d5-4512-b7d4-78933d76b5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_text_from_folder(folder_path):\n",
    "    combined_text = \"\"\n",
    "    folder_path = Path(folder_path)\n",
    "\n",
    "    if not folder_path.is_dir():\n",
    "        print(\"folder not found\")\n",
    "        return \"\"\n",
    "\n",
    "    pdf_files = list(folder_path.glob('*.pdf'))\n",
    "    html_files = list(folder_path.glob('*.html'))\n",
    "\n",
    "    if not pdf_files and not html_files:\n",
    "        print(\"no PDF or HTML files found\")\n",
    "        return \"\"\n",
    "\n",
    "        \n",
    "    for pdf_file in pdf_files:\n",
    "        combined_text += f\"\\n\\n{'='*15} START OF DOCUMENT: {pdf_file.name} {'='*15}\\n\\n\"\n",
    "        pdf_content = extract_text_from_pdf(pdf_file)\n",
    "        combined_text += pdf_content\n",
    "        combined_text += f\"\\n\\n{'='*15} END OF DOCUMENT: {pdf_file.name} {'='*15}\\n\\n\"\n",
    "\n",
    "    for html_file in html_files:\n",
    "        combined_text += f\"\\n\\n{'='*15} START OF DOCUMENT: {html_file.name} {'='*15}\\n\\n\"\n",
    "        html_content = extract_text_from_html(html_file) \n",
    "        combined_text += html_content\n",
    "        combined_text += f\"\\n\\n{'='*15} END OF DOCUMENT: {html_file.name} {'='*15}\\n\\n\"\n",
    "\n",
    "    return combined_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76858d7-22e3-428d-a7d0-e53962aad265",
   "metadata": {},
   "source": [
    "### configuring LLM api and creating a field list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "02e613eb-bdd3-4993-a771-722936cce571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemini Key configured\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    api_key = 'your-api-key'\n",
    "    genai.configure(api_key=api_key)\n",
    "    print('Gemini Key configured')\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "fields_to_extract = [\n",
    "    \"Bid Number\", \"Title\", \"Due Date\", \"Bid Submission Type\", \"Term of Bid\",\n",
    "    \"Pre Bid Meeting\", \"Installation\", \"Bid Bond Requirement\", \"Delivery Date\",\n",
    "    \"Payment Terms\", \"Any Additional Documentation Required\", \"MFG for Registration\",\n",
    "    \"Contract or Cooperative to use\", \"Model_no\", \"Part_no\", \"Product\",\n",
    "    \"contact_info\", \"company_name\", \"Bid Summary\", \"Product Specification\"\n",
    "]\n",
    "\n",
    "field_list_str = \"\\n\".join([f\"- {field}\" for field in fields_to_extract])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090c95aa-9f6f-4bed-8e00-6ee79a6deef5",
   "metadata": {},
   "source": [
    "###  function to implement the prompt and grabbing the response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "581ffd93-e504-4159-b187-454d69a7658e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_json_from_llm(context_text, field_list):\n",
    "    \n",
    "    prompt = prompt = f\"\"\"\n",
    "    Analyze the following combined text from multiple procurement-related documents (like RFPs, amendments, Q&As, spec sheets, web pages). Your task is to extract specific information into a strict JSON format, synthesizing details accurately and handling potential inconsistencies.\n",
    "\n",
    "    **Document Context:** The text includes content from different files, marked by 'START OF DOCUMENT: [Filename]' and 'END OF DOCUMENT: [Filename]'. Filenames or content might indicate updates (e.g., 'Addendum', 'Amendment', 'Clarification', 'Q&A', dates).\n",
    "\n",
    "    **CRITICAL RULES FOR ACCURATE & ROBUST EXTRACTION:**\n",
    "    1.  **Identify Updates:** Carefully examine all documents. Identify any that serve as updates, corrections, or clarifications to earlier documents. Look for explicit keywords ('Addendum', 'Amendment', 'Update', 'Correction', 'Response to Questions') or infer based on context and dates.\n",
    "    2.  **Prioritize Latest Information:** If conflicting information exists for *any* field (e.g., different due dates, modified specs, updated contact info), you **MUST** use the information from the document identified as the most recent or superseding update as the final, correct value. Discard the older, conflicting information.\n",
    "    3.  **Synthesize Comprehensively:** Combine all relevant, non-conflicting details from *all* provided documents to create a complete answer for each field. For example:\n",
    "        * `Installation`: Gather requirements like 'white glove service' from one document and specific tasks like 'etching' or 'Autopilot setup' from another.\n",
    "        * `Product Specification`: Consolidate base requirements from the main RFP with detailed specs from separate sheets and any modifications mentioned in updates.\n",
    "    4.  **Strict JSON Output:** Return **ONLY** a single, valid JSON object. Adhere *exactly* to the requested field names. Do not include ```json markdown, introductory/closing text, apologies, or any explanations outside the JSON structure. Use double quotes for all keys and string values.\n",
    "    5.  **Handle Missing Data:** If, after reviewing *all* documents, definitive information for a field cannot be found, use the JSON value `null`. **Do not guess, infer, or fabricate information.** If a field seems partially mentioned but isn't clear, prefer `null`.\n",
    "    6.  **Field-Specific Guidance:**\n",
    "        * `Bid Number`: Find the primary identifier (e.g., RFP No., PORFP #, Solicitation ID). Look for patterns like `JA-XXXXXX` or `E20PXXXXXXX`.\n",
    "        * `Due Date`: Extract the *final, effective* deadline considering all updates. Include time and timezone if specified.\n",
    "        * `Model_no` / `Part_no`: Extract only if the documents explicitly state a *required* Model or Part Number/SKU as the *subject* of the procurement (e.g., \"This RFP is for the purchase of Dell Latitude 5550\"). If the documents only list *minimum specs* or examples, use `null`.\n",
    "        * `Product`: Briefly list the main items/services requested. Mention specific required models if identified per the `Model_no` rule.\n",
    "        * `Product Specification`: Summarize key requirements and specs. Structure clearly, perhaps using nested objects for different product types/tiers. Include details from dedicated spec sheets if present.\n",
    "        * `Any Additional Documentation Required`: List *specific* forms, affidavits, certifications, or attachments vendors must submit.\n",
    "\n",
    "    **FIELDS TO EXTRACT:**\n",
    "    {field_list}\n",
    "\n",
    "    **COMBINED DOCUMENT TEXT:**\n",
    "    ```text\n",
    "    {context_text}\n",
    "    ```\n",
    "\n",
    "    **JSON OUTPUT (Strictly JSON, starting with {{ and ending with }}):**\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        model = genai.GenerativeModel('models/gemini-2.5-pro')\n",
    "        response = model.generate_content(\n",
    "            prompt,\n",
    "            generation_config=genai.types.GenerationConfig(\n",
    "                 response_mime_type=\"application/json\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "        cleaned_text = response.text.strip().removeprefix('```json').removesuffix('```').strip()\n",
    "        return cleaned_text\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b230ca76-35b5-4bf1-bfa9-dadd84b35d24",
   "metadata": {},
   "source": [
    "### creating json for Bid1 folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bb4effc1-1ba2-4748-9bc2-f2e541aeea31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Saved\n"
     ]
    }
   ],
   "source": [
    "folder_to_process = './data/Bid1'\n",
    "output_filename = 'Bid1_output.json'\n",
    "\n",
    "\n",
    "all_text = get_all_text_from_folder(folder_to_process)\n",
    "\n",
    "extracted_json_string = None\n",
    "extracted_json_string = get_json_from_llm(all_text, field_list_str)\n",
    "\n",
    "try:\n",
    "    json_data = json.loads(extracted_json_string)\n",
    "    with open(output_filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(json_data, f, indent=2, ensure_ascii=False)\n",
    "        print(\"File Saved\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf71975-7bbe-4f3a-9629-b6e45e624e30",
   "metadata": {},
   "source": [
    "### creating json for Bid2 folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9edeadb6-b343-409e-99e3-c86f111a75e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Saved\n"
     ]
    }
   ],
   "source": [
    "folder_to_process = './data/Bid2'\n",
    "output_filename = 'Bid2_output.json'\n",
    "\n",
    "\n",
    "all_text = get_all_text_from_folder(folder_to_process)\n",
    "\n",
    "extracted_json_string = None\n",
    "extracted_json_string = get_json_from_llm(all_text, field_list_str)\n",
    "\n",
    "try:\n",
    "    json_data = json.loads(extracted_json_string)\n",
    "    with open(output_filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(json_data, f, indent=2, ensure_ascii=False)\n",
    "        print(\"File Saved\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2a4bbe-5afe-4100-88be-849884a7dade",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
